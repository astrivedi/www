<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  <title>
    
      Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations · Ashutosh Trivedi
    
  </title>

  <meta name="description"
        content="My research develops formal methods for reinforcement learning and trustworthy AI,  with a focus on verification and accountability in high-stakes decision-making systems.
"/>

  <link rel="stylesheet" href="/assets/css/main.css"/>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Ashutosh Trivedi",
    "url": "https://ashutoshtrivedi.com",
    "image": "https://ashutoshtrivedi.com/assets/img/portrait.jpg",
    "jobTitle": "Associate Professor of Computer Science",
    "affiliation": {
      "@type": "Organization",
      "name": "University of Colorado Boulder",
      "url": "https://www.colorado.edu"
    },
    "alumniOf": [
      {
        "@type": "CollegeOrUniversity",
        "name": "University of Warwick"
      }
    ],
    "sameAs": [
      "https://www.wikidata.org/wiki/Q102112267",
      "https://scholar.google.com/citations?user=9WDXyy4AAAAJ",
      "https://orcid.org/0000-0001-9346-0126",
      "https://dblp.org/pid/06/5756.html"
    ]
  }
  </script>
</head>

  <body>
    <div class="container">
      <nav class="nav">
<a class="brand home-icon" href="/" aria-label="Home">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"
       width="22" height="22" fill="none"
       stroke="currentColor" stroke-width="2"
       stroke-linecap="round" stroke-linejoin="round">
    <path d="M3 10.5L12 3l9 7.5"/>
    <path d="M5 10v10h5v-6h4v6h5V10"/>
  </svg>
</a>

  <ul>
    <li><a href="/research/">Research</a></li>
    <li><a href="/publications/">Publications</a></li>
    <li><a href="/group/">Group</a></li>
    <li><a href="/teaching/">Teaching</a></li>
     <li><a href="/bio/">Bio</a></li>
    <li><a href="/contact/">Contact</a></li>
  </ul>
</nav>

      <article class="paper-page">
  <header>
    <h1>Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations</h1>



    
      <p><strong>Authors:</strong> Ranit Akash, Ashish Kumar, Verya Monjezi, Ashutosh Trivedi, Gang Tan, Saeid Tizpaz-Niari</p>
    

    <p>
      <strong>ASE 2025</strong>
       (2025)
    </p>

    
      <p class="paper-tags">
        
          <a class="tag" href="/tags/accountablese/">accountableSE</a>
        
          <a class="tag" href="/tags/trustworthyai/">trustworthyAI</a>
        
          <a class="tag" href="/tags/ai/">ai</a>
        
          <a class="tag" href="/tags/top/">top</a>
        
      </p>
    

    <p class="paper-links">
      <a href="/assets/papers/2025-ase-discrimination-clusters.pdf">PDF</a>
       · <a href="/assets/papers/2025-ase-discrimination-clusters.bib" download>BibTeX</a>
       · <a href="/assets/papers/2025-ase-discrimination-clusters.pptx">Slides</a>
      
      
      
      
    </p>
  </header>

  
  <section>
  <h2>Abstract</h2>
  <p>Fairness in algorithmic decision-making is often framed in terms of individual fairness, which requires that similar individuals receive similar outcomes. A system violates individual fairness if there exists a pair of inputs differing only in protected attributes (such as race or gender) that lead to significantly different outcomes, one favorable and the other unfavorable. While this notion highlights isolated instances of unfairness, it fails to capture broader patterns of clustered discrimination that may affect entire subgroups.</p>

<p>We introduce and motivate the concept of discrimination clustering, a generalization of individual fairness violations. Rather than detecting single counterfactual disparities, we seek to uncover regions of the input space where small perturbations in protected features lead to k-significantly distinct clusters of outcomes. That is, for a given input, we identify a local neighborhood, differing only in protected attributes, whose members’ outputs separate into many distinct clusters. These clusters reveal arbitrariness in treatment based solely on protected attributes, exposing patterns of algorithmic bias that elude pairwise fairness checks.</p>

<p>We present HYFAIR, a hybrid technique that combines formal symbolic analysis (via SMT and MILP solvers) to certify individual fairness with randomized search to discover discriminatory clusters. This combination enables both formal guarantees when no counterexamples exist and the detection of severe violations that are computationally challenging for symbolic methods alone. Given a set of inputs exhibiting high k-discrimination, we further introduce a novel explanation method that generates interpretable, decision-tree-style artifacts.</p>

<p>Our experiments show that HYFAIR outperforms state-of-the-art fairness verification and local explanation methods. It reveals that some benchmarks exhibit substantial discrimination clustering, while others show limited or no disparities with respect to protected attributes. It also provides intuitive explanations that support understanding and mitigation of unfairness.</p>

</section>

  

  <section>
    

  </section>
</article>

      <footer class="footer">
  © 2025 Ashutosh Trivedi · University of Colorado Boulder
</footer>

    </div>
  </body>
</html>
