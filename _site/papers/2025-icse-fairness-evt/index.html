<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  <title>
    
      Fairness Testing through Extreme Value Theory · Ashutosh Trivedi
    
  </title>

  <meta name="description"
        content="My research develops formal methods for reinforcement learning and trustworthy AI,  with a focus on verification and accountability in high-stakes decision-making systems.
"/>

  <link rel="stylesheet" href="/assets/css/main.css"/>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Ashutosh Trivedi",
    "url": "https://ashutoshtrivedi.com",
    "image": "https://ashutoshtrivedi.com/assets/img/portrait.jpg",
    "jobTitle": "Associate Professor of Computer Science",
    "affiliation": {
      "@type": "Organization",
      "name": "University of Colorado Boulder",
      "url": "https://www.colorado.edu"
    },
    "alumniOf": [
      {
        "@type": "CollegeOrUniversity",
        "name": "University of Warwick"
      }
    ],
    "sameAs": [
      "https://www.wikidata.org/wiki/Q102112267",
      "https://scholar.google.com/citations?user=9WDXyy4AAAAJ",
      "https://orcid.org/0000-0001-9346-0126",
      "https://dblp.org/pid/06/5756.html"
    ]
  }
  </script>
</head>

  <body>
    <div class="container">
      <nav class="nav">
<a class="brand home-icon" href="/" aria-label="Home">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"
       width="22" height="22" fill="none"
       stroke="currentColor" stroke-width="2"
       stroke-linecap="round" stroke-linejoin="round">
    <path d="M3 10.5L12 3l9 7.5"/>
    <path d="M5 10v10h5v-6h4v6h5V10"/>
  </svg>
</a>

  <ul>
    <li><a href="/research/">Research</a></li>
    <li><a href="/publications/">Publications</a></li>
    <li><a href="/group/">Group</a></li>
    <li><a href="/teaching/">Teaching</a></li>
     <li><a href="/bio/">Bio</a></li>
    <li><a href="/contact/">Contact</a></li>
  </ul>
</nav>

      <article class="paper-page">
  <header>
    <h1>Fairness Testing through Extreme Value Theory</h1>



    
      <p><strong>Authors:</strong> Verya Monjezi, Ashutosh Trivedi, Vladik Kreinovich, Saeid Tizpaz-Niari</p>
    

    <p>
      <strong>ICSE '25: Proceedings of the IEEE/ACM 47th International Conference on Software Engineering </strong>
       (2025)
    </p>

    
      <p class="paper-tags">
        
          <a class="tag" href="/tags/accountablese/">accountableSE</a>
        
      </p>
    

    <p class="paper-links">
      <a href="/assets/papers/2025-icse-fairness-evt.pdf">PDF</a>
       · <a href="/assets/papers/2025-icse-fairness-evt.bib" download>BibTeX</a>
      
      
      
      
      
    </p>
  </header>

  
  <section>
  <h2>Abstract</h2>
  <p>Data-driven software is increasingly being used as a critical component of automated decision-support systems. Since this class of software learns its logic from historical data, it can encode or amplify discriminatory practices. Previous research on algorithmic fairness has focused on improving “average-case” fairness. On the other hand, fairness at the extreme ends of the spectrum, which often signifies lasting and impactful shifts in societal attitudes, has received significantly less emphasis.</p>

<p>Leveraging the statistics of extreme value theory (EVT), we propose a novel fairness criterion called extreme counterfactual discrimination (ECD). This criterion estimates the worst-case amounts of disadvantage in outcomes for individuals solely based on their memberships in a protected group. Utilizing tools from search-based software engineering and generative AI, we present a randomized algorithm that samples a statistically significant set of points from the tail of ML outcome distributions even if the input dataset lacks a sufficient number of relevant samples.</p>

<p>We conducted several experiments on four ML models (deep neural networks, logistic regression, and random forests) over 10 socially relevant tasks from the literature on algorithmic fairness. First, we evaluate the generative AI methods and find that they generate sufficient samples to infer valid EVT distribution in 95% of cases. Remarkably, we found that the prevalent bias mitigators reduce the average-case discrimination but increase the worst-case discrimination significantly in 35% of cases. We also observed that even the tail-aware mitigation algorithm—MiniMax-Fairness—increased the worst-case discrimination in 30% of cases. We propose a novel ECD-based mitigator that improves fairness in the tail in 90% of cases with no degradation of the average-case discrimination. We hope that the EVT framework serves as a robust tool for evaluating fairness in both average-case and worst-case discrimination.</p>

</section>

  

  <section>
    

  </section>
</article>

      <footer class="footer">
  © 2025 Ashutosh Trivedi · University of Colorado Boulder
</footer>

    </div>
  </body>
</html>
