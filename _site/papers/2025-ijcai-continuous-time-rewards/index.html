<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  <title>
    
      Continuous-Time Reward Machines · Ashutosh Trivedi
    
  </title>

  <meta name="description"
        content="My research develops formal methods for reinforcement learning and trustworthy AI,  with a focus on verification and accountability in high-stakes decision-making systems.
"/>

  <link rel="stylesheet" href="/assets/css/main.css"/>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Ashutosh Trivedi",
    "url": "https://ashutoshtrivedi.com",
    "image": "https://ashutoshtrivedi.com/assets/img/portrait.jpg",
    "jobTitle": "Associate Professor of Computer Science",
    "affiliation": {
      "@type": "Organization",
      "name": "University of Colorado Boulder",
      "url": "https://www.colorado.edu"
    },
    "alumniOf": [
      {
        "@type": "CollegeOrUniversity",
        "name": "University of Warwick"
      }
    ],
    "sameAs": [
      "https://www.wikidata.org/wiki/Q102112267",
      "https://scholar.google.com/citations?user=9WDXyy4AAAAJ",
      "https://orcid.org/0000-0001-9346-0126",
      "https://dblp.org/pid/06/5756.html"
    ]
  }
  </script>
</head>

  <body>
    <div class="container">
      <nav class="nav">
<a class="brand home-icon" href="/" aria-label="Home">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"
       width="22" height="22" fill="none"
       stroke="currentColor" stroke-width="2"
       stroke-linecap="round" stroke-linejoin="round">
    <path d="M3 10.5L12 3l9 7.5"/>
    <path d="M5 10v10h5v-6h4v6h5V10"/>
  </svg>
</a>

  <ul>
    <li><a href="/research/">Research</a></li>
    <li><a href="/publications/">Publications</a></li>
    <li><a href="/group/">Group</a></li>
    <li><a href="/teaching/">Teaching</a></li>
     <li><a href="/bio/">Bio</a></li>
    <li><a href="/contact/">Contact</a></li>
  </ul>
</nav>

      <article class="paper-page">
  <header>
    <h1>Continuous-Time Reward Machines</h1>



    
      <p><strong>Authors:</strong> Amin Falah, Shibashis Guha, Ashutosh Trivedi</p>
    

    <p>
      <strong>Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, {IJCAI} 2025, Montreal, Canada, August 16-22, 2025</strong>
       (2025)
    </p>

    
      <p class="paper-tags">
        
          <a class="tag" href="/tags/formalrl/">formalRL</a>
        
      </p>
    

    <p class="paper-links">
      <a href="/assets/papers/2025-ijcai-continuous-time-rewards.pdf">PDF</a>
       · <a href="/assets/papers/2025-ijcai-continuous-time-rewards.bib" download>BibTeX</a>
      
      
       · <a href="https://www.ijcai.org/proceedings/2025/563">arXiv</a>
      
      
    </p>
  </header>

  
  <section>
  <h2>Abstract</h2>
  <p>Reinforcement Learning (RL) is a sampling-based method for sequential decision-making, in which a learning agent iteratively converges toward an optimal policy by leveraging feedback from the environment in the form of scalar reward signals. While timing information is often abstracted in discrete-time domains, time-critical learning applications—such as queuing systems, population processes, and manufacturing systems—are naturally modeled as Continuous-Time Markov Decision Processes (CTMDPs). Since the seminal work of Bradtke and Duff, model-free RL for CTMDPs has become well-understood. However, in many practical applications, practitioners possess high-quality information about system rates derived from traditional queuing theory, which learning agents could potentially exploit to accelerate convergence. Despite this, classical RL algorithms for CTMDPs typically re-learn these parameters through sampling. In this work, we propose continuous-time reward machines (CTRMs), a novel framework that embeds reward functions and real-time state-action dynamics into a unified structure. CTRMs enable RL agents to effectively navigate dense-time environments while leveraging reward shaping and counterfactual experiences for accelerated learning. Our empirical results demonstrate CTRMs’ ability to improve learning efficiency in time-critical environments.</p>

</section>

  

  <section>
    

  </section>
</article>

      <footer class="footer">
  © 2025 Ashutosh Trivedi · University of Colorado Boulder
</footer>

    </div>
  </body>
</html>
